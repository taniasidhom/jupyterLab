{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../imgs/CampQMIND_banner.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process <a class=\"anchor\" id=\"top\"></a>\n",
    "\n",
    "Author: [Sydney Corbett](https://www.linkedin.com/in/sydneycorbett/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview Video - Watch! This! First!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"https://www.youtube.com/embed/A-TqcH56UvU\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1ee3ae9d208>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame, Image\n",
    "IFrame('https://www.youtube.com/embed/A-TqcH56UvU',560,315)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More In Depth Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"https://www.youtube.com/embed/KovN7WKI9Y0\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x11074a850>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame('https://www.youtube.com/embed/KovN7WKI9Y0', 560,315)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process ... Let's get mathy \n",
    "\n",
    "Four components of the process ...\n",
    "1. Set of states within specified environment - S\n",
    "2. Set of possible agents actions that can be performed within our environment - A\n",
    "3. A description of each action's effect on the current state - T\n",
    "4. A function that gives a reward based on the desired state and action - R(s,a)\n",
    "\n",
    "Then there's the policy that seeks to solve the MDP ie. indicates the best action 'a' to take while in state S\n",
    "\n",
    "Taker a deer dive ...\n",
    "\n",
    "Transition Model: T(S,a,S')˜ P(S'|S,a) ... in english: in state S, take action a, agent now in potentially new state S, P is probability of getting to S' given S, a\n",
    "\n",
    "Markov Propertly State: P[St+1|a, S0, ...St] = P[St+1|a,St]... S+1 is the future state and S0,...,St is all previous states with St being the last state the agent was in. NEW STATE DEPENDS ONLY ON THAT ONE PREVIOUS STATE ST !\n",
    "\n",
    "Transition Probability: Probability of going to each different state. Without going into too much detail on matricies, to calculate the probability of going to each state you use vector/matrix math to calculate the odds of each state by taking the dot products\n",
    "\n",
    "Regular Markov Chain: Finding the probability of the potential states given the initial probabilites without having defined a single starting state by repeatedly using dot products ... \n",
    "\n",
    "u ^(n) = uP^(n) ... u - vector representing initial distribution of states, P - transition matrix for a Markov chain, n - number of states\n",
    "\n",
    "Now, how does this all lead to the optimal solution ...\n",
    "\n",
    "Reward and Return Value: agent wants to maximize total sum of future reward - we call this return R and it's modelled using reward (r)\n",
    "\n",
    "Rt = (sum from k = 0 to infinity) rt+k+1 ... the t+k+1 is subscript\n",
    "\n",
    "However, more commonly used is the FUTURE CUMULATIVE DISCOUNTED REWARD\n",
    "\n",
    "Rt = (sum from k = 0 to infinity) (gamma^k)(rt+k+1 .. .again the t+k+1 is subscript)\n",
    "\n",
    "The 'gamma' is the discount factor and is between 0 and 1. As you might be able to tell, a smaller discount factor puts more emphasis on earlier rewards and a discount factor closer to 1 puts more emphasis on later rewards.\n",
    "\n",
    "More on the Policy Function: π(s,a) - takes current state 's', agent action 'a'and returns the probability of taking that action in that state based off reward\n",
    "\n",
    "Now we add in value functions where we use the reward to calculate expected reward...\n",
    "\n",
    "Two types in reinforcement learning. \n",
    "\n",
    "1. State value function: returns expected reward when we start initially at state s and act within policy rules ... Vπ(s) = Eπ[Rt|st=s]\n",
    "\n",
    "2. Action value function: returns expected reward of taking an action 'a' in a given state 's' when following the policy ... Qπ(s,a) = Eπ[Rt|st=s, at=a]\n",
    "\n",
    "From here we have optimal value fucntions ...\n",
    "\n",
    "1. Optimal state value function: Vπ*(s) = maxπVπ(s) ... maximizes expected reward over all policies\n",
    "2. Optimal action value function: Qπ*(s,a) = maxπQπ(s,a) ... maximizes action value function over all policies\n",
    "\n",
    "When selecting an action the environment returns the next state, so there is randomness in the returned state ... that's where the Bellman function comes in. To learn more about this just look up the Bellman function / equation!\n",
    "\n",
    "Anyways, all that math is to say that there are states and agents and using reward based on the outcome of going to a next state given the goal to get to a desired state a series of probabilities and policies get created that eventually lead to 'learning' of what the best sequence of steps to take is given the current state ie. the optimal policy. Take a deep breath. I know that was a lot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pros and Cons\n",
    "Pros: \n",
    "    - adaptive to changing and unknown characteristics of the environment\n",
    "    - good for use in cases where you need to develop an optimal policy\n",
    "Cons:\n",
    "    - time consuming\n",
    "    - relatively complicated code\n",
    "    - may execute unexpected behavours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation in python\n",
    "Finally the part you've all been waiting for ... congrats on making it through that math reading!\n",
    "\n",
    "If you want to use a toolbox, the one you're going to want to use is called mdptoolbox ...  check this link out for more details https://pymdptoolbox.readthedocs.io/en/latest/api/mdptoolbox.html. There are many modules associated with this toolbox for example an mdp module, util module, example module, which can all be adapted for your needs.\n",
    "\n",
    "However, your project might require code from scratch. \n",
    "\n",
    "Consider the following example. You have a 3x4 grid and you can go either N, E, S, W. Each box of the grid has a number representing a state and you want to get to state 10 the fastest way possible ie. you need to come up with a policy that leads to best reward, and that's where the MDP coemes in.\n",
    "\n",
    "The example being referring to is linked here: https://g-stat.com/using-markov-decision-process-mdp-to-create-a-policy-hands-on-python-example/, and you will be able to see the grid probabilities after each iteration. \n",
    "\n",
    "In the example there is a link to the 'World' class, which is essentially the environment he's working with, and hte 'main' class which holds all the tools for the MDP. To get an indepth understanding of what is required I highly recommend going through and playing around with the code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Return to top](#top)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
